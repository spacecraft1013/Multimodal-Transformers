pipe-parallel-size: 1
tensor-model-parallel-size: 5

num-layers: 32
hidden-size: 2560
num-attention-heads: 32
seq-length: 2048
max-position-embeddings: 2048
norm: layernorm
pos-emb: rotary
attn-mask-type: local
no-weight-tying: True

tokenizer-type: GPT2BPETokenizer
vocab-file: data/vocab.json
merge-file: data/merges.txt

save: checkpoints
load: checkpoints

tensorboard-dir: tensorboard
log-dir: logs

scaled-upper-triang-masked-softmax-fusion: False
bias-gelu-fusion: False

optimizer: Adam
lr: 0.00016
weight-decay: 0

zero-optimization:
  stage: 1
  allgather-partitions: True
  allgather-bucket-size: 500000000
  overlap-comm: True
  reduce-scatter: True
  reduce-bucket-size: 500000000
  contiguous-gradients: True
  cpu-offload: False

micro-batch-size: 4

checkpoint-activations: True
checkopint-num-layers: 1
partition-activations: True
sychronize-each-layer: True

gradient-clipping: 1.0
hidden-dropout: 0
attention-dropout: 0

fp16:
  fp16: True
  enabled: True
  loss-scale: 0
  loss-scale-window: 1000
  hysteresis: 2
  min-loss-scale: 1

train-iters: 320000
lr-decay-iters: 320000
distributed-backend: nccl
lr-decay-style: cosine
lr-warmup-fraction: 0.01
save-interval: 10000
eval-interval: 1000
eval-iters: 10
dataloader-type: cyclic

log-interval: 100
steps-per-print: 10
keep-last-n-checkpoints: 10
wall-clock-breakdown: False

img-dim: 256
patch-dim: 16

multimodal-datasets:
  multimodal-sampler: True
  text-dataset:
    type: pile
    dir: /data/ambouk3/datasets/pile/pile/pile_text_document
    data-impl: mmap
    splits: 949,50,1
  image-dataset:
    type: imagenet
    dir: /data/ambouk3/datasets/imagenet