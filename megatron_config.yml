pipe-parallel-size: 2
tensor-model-parallel-size: 2

num-layers: 32
hidden-size: 2560
num-attention-heads: 32
seq-length: 2048
max-position-embeddings: 2048
norm: layernorm
pos-emb: rotary
attn-mask-type: local
no-weight-tying: True

tokenizer-type: GPT2BPETokenizer
vocab-file: data/vocab.json
merge-file: data/merges.txt

save: checkpoints
load: checkpoints

tensorboard-dir: tensorboard
log-dir: logs

scaled-upper-triang-masked-softmax-fusion: False
bias-gelu-fusion: False

optimizer:
  type: Adam
  params:
    lr: 0.00016
    betas: [0.9, 0.999]
    eps: 1.0e-8

zero-optimization:
  stage: 1
  allgather-partitions: True
  allgather-bucket-size: 500000000
  overlap-comm: True
  reduce-scatter: True
  reduce-bucket-size: 500000000
  contiguous-gradients: True
  cpu-offload: False

micro-batch-size: 4

checkpoint-activations: True
checkopint-num-layers: 1
partition-activations: True
sychronize-each-layer: True

gradient-clipping: 1.0
weight-decay: 0
hidden-dropout: 0
attention-dropout: 0

fp16:
  fp16: True
  enabled: True
  loss-scale: 0
  loss-scale-window: 1000
  hysteresis: 2
  min-loss-scale: 1

train-iters: 320000
lr-decay-iters: 320000
distributed-backend: nccl
lr-decay-style: cosine
lr-warmup-fraction: 0.01
save-interval: 10000
eval-interval: 1000
eval-iters: 10

log-interval: 100
steps-per-print: 10
keep-last-n-checkpoints: 10
wall-clock-breakdown: False

img-dim: 256
patch-dim: 16

multimodal-datasets:
  data-dir: data/
  num-preprocessing-workers: -1
  imagenet-dir: B:/Imagenet
  image-size: 256
  wikitext-dir: data/WikiText
  wikitext-dataset: wikitext.pth